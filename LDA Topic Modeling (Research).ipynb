{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4d88ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      use quite time also member decide would good w...\n",
      "1      able currently cache sadly keep charge tell do...\n",
      "2      Great little buggy moment load full zoom trail...\n",
      "3      need cash phone please want cash away back cas...\n",
      "4      Theres literally three entire city big one cou...\n",
      "                             ...                        \n",
      "195    okay prefer different find higher difficult ra...\n",
      "196    Caches free website consider pay original didn...\n",
      "197    Used enjoy occasionally however almost need ac...\n",
      "198    really engage still need careful go think need...\n",
      "199    open every week user interaction cause whateve...\n",
      "Name: text_lemmatized, Length: 200, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Computing Analytics Reasearch--Topic Modeling\n",
    "#Author: Nikki Kudamik\n",
    "#Date: 2/22/24\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk #make sure to run: nltk.download('stopwords') in another cell or before running program\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer #make sure to run: nltk.download('wordnet'), nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "#must be declared before FREQWORDS global variable\n",
    "cnt = Counter()\n",
    "for text in df[\"text_wo_stop\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "cnt.most_common(10)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "\n",
    "pd.options.mode.chained_assignment = None #??\n",
    "\n",
    "#FUNCTION DEFINITIONS\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "def stem_words(text):\n",
    "    \"\"\"custom function to reduce words to their root form\"\"\"\n",
    "    return \" \".join([stemmer.stem(word) for word in str(text).split()])\n",
    "def remove_smallwords(text):\n",
    "    \"\"\"custom function to remove words that consist of less than three letters\"\"\"\n",
    "    return re.sub(r'\\b\\w{1,3}\\b', '', text)\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "def clean_reviews(reviews):\n",
    "    #call remove_punctuation()\n",
    "    df[\"text_wo_punct\"] = reviews.apply(lambda text:remove_punctuation(text))\n",
    "    \n",
    "    #call to remove_stopwords()\n",
    "    df[\"text_wo_stop\"] = df[\"text_wo_punct\"].apply(lambda text:remove_stopwords(text))\n",
    "\n",
    "    #call to remove_freqwords()\n",
    "    df[\"text_wo_freq\"] = df[\"text_wo_stop\"].apply(lambda text:remove_freqwords(text))\n",
    "    \n",
    "    #call to remove_smallwords\n",
    "    df[\"text_wo_smallwords\"] = df[\"text_wo_freq\"].apply(lambda text:remove_smallwords(text))\n",
    "    \n",
    "    #call to lemmatize_words()\n",
    "    df[\"text_lemmatized\"] =  df[\"text_wo_smallwords\"].apply(lambda text:lemmatize_words(text))\n",
    "    \n",
    "    return df[\"text_lemmatized\"]    \n",
    "\n",
    "#MAIN PROGRAM\n",
    "def main():\n",
    "\n",
    "    #manually change dataset.xlsx to dataset.csv\n",
    "    full_df = pd.read_csv(\"dataset.csv\")\n",
    "    df = full_df[[\"review\"]] #insert column name in \"\", so in this case it is \"review\"\n",
    "    df[\"review\"] = df[\"review\"].astype(str)\n",
    "    #full_df.head()\n",
    "\n",
    "    #numpy array OPTION 1\n",
    "        #reviews = df[\"review\"].to_numpy() #array of reviews\n",
    "        #print(reviews)\n",
    "\n",
    "    #numpy array OPTION 2\n",
    "    reviews = df[\"review\"]\n",
    "    #reviews_array = np.array(reviews)\n",
    "    #print(reviews_array)\n",
    "\n",
    "    #would it be possible to make the reviews df to a numpy array after the processing done in clean_reviews?\n",
    "    processed_reviews = clean_reviews(reviews) #.to_numpy()\n",
    "\n",
    "    \n",
    "    print(processed_reviews)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bba9d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nikki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188aea29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
